# gesture-recognition-system

# ğŸ¤– Real-Time Hand Gesture Recognition with Voice Feedback

This project uses **MediaPipe**, **TensorFlow**, and **OpenCV** to recognize hand gestures from a webcam in real-time. It also provides **audio feedback** using a Text-to-Speech (TTS) engine to announce the detected gesture.

Watch the code run here: https://www.linkedin.com/posts/adityasinha2006_machinelearning-computervision-ai-activity-7345395881346527233-9J2r?utm_source=share&utm_medium=member_desktop&rcm=ACoAAFIBTYABLifzvbm0bR14dN22zxSvlZjLNCs

---

## ğŸ¯ Project Features

ğŸ‘‰ Real-time hand gesture detection using your webcam  
ğŸ‘‰ Detects both left and right hands  
ğŸ‘‰ Classifies gestures using a trained deep learning model (TensorFlow)  
ğŸ‘‰ Announces detected gestures out loud with offline TTS  
ğŸ‘‰ Compatible with **Windows**, **Linux**, and **macOS** (Linux requires `espeak`)  
ğŸ‘‰ Cooldown logic prevents repeated announcements when the same gesture is held  
ğŸ‘‰ Well-structured, beginner-friendly, and easy-to-understand code  
ğŸ‘‰ `data_app.py` automates the complete pipeline â€” collects data, prepares dataset, trains model â€” in order  

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/EngineerAditya/gesture-recognition-system.git
cd gesture-recognition-system
```

### 2. Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 3. Additional Setup for Linux Users (Text-to-Speech)

On Linux, `pyttsx3` relies on `espeak` for speech synthesis. Install it using:

```bash
sudo apt update
sudo apt install espeak
```

### 4. Collect Data, Prepare Dataset & Train the Model (One-Click)

Simply run:

```bash
python data_app.py
```

This will:

âœ… Launch the data collection script  
âœ… Automatically combine the dataset  
âœ… Train the TensorFlow model and save it  

Your trained model (`models/tf_gesture_model.h5`) and label encoder (`models/label_encoder.pkl`) will be ready for use.

---

## ğŸ® Running Real-Time Gesture Recognition

Make sure your webcam is working and run:

```bash
python predict_live.py
```

### Controls:

- Press **'q'** to quit  
- Detected gestures will appear on the screen  
- The gesture will be spoken aloud using TTS  

---

## ğŸ”Š Text-to-Speech Details

- Uses `pyttsx3` for offline TTS (cross-platform)  
- On Linux, requires `espeak` system package  
- Cooldown of **3 seconds** prevents constant repetition  
- Announces new gestures immediately  

---

## ğŸ“¦ Project Structure

```
gesture-recognition-system/
â”œâ”€â”€ data/                  # Collected data and processed CSVs
â”œâ”€â”€ models/                # Saved TensorFlow model & label encoder
â”œâ”€â”€ src/                   # Data collection & preparation scripts
â”œâ”€â”€ data_app.py            # Automates collect, prepare & train steps
â”œâ”€â”€ predict_live.py        # Real-time gesture recognition script
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # Project overview and instructions
```

---

## ğŸ§‘â€ğŸ’» Technologies Used

- [MediaPipe](https://developers.google.com/mediapipe) â€” Hand landmark detection  
- [OpenCV](https://opencv.org/) â€” Video processing  
- [TensorFlow](https://www.tensorflow.org/) â€” Gesture classification  
- [pyttsx3](https://pypi.org/project/pyttsx3/) â€” Offline text-to-speech  
- [Pandas & NumPy](https://pandas.pydata.org/) â€” Data handling  

---

## ğŸ’¡ Future Improvements

- More complex deep learning models for higher accuracy  
- Larger, more diverse gesture dataset  
- Optional integration with `gTTS` for natural-sounding speech (requires internet)  
- GUI-based interaction for non-technical users  

---

## âœ¨ Credits

I have used **ChatGPT** to structure and make this project code clean, readable, and beginner-friendly. ğŸ˜

---

## ğŸ¤ Contributions

Feel free to fork, suggest improvements, or open pull requests. All contributions are welcome!
